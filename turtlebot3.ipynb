{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!sudo pip install --extra-index-url https://rospypi.github.io/simple/ rospy rosbag","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:48:25.659489Z","iopub.execute_input":"2022-02-11T21:48:25.659888Z","iopub.status.idle":"2022-02-11T21:48:42.796172Z","shell.execute_reply.started":"2022-02-11T21:48:25.659779Z","shell.execute_reply":"2022-02-11T21:48:42.795214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rosbag-metadata","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:48:42.800257Z","iopub.execute_input":"2022-02-11T21:48:42.800501Z","iopub.status.idle":"2022-02-11T21:48:53.917684Z","shell.execute_reply.started":"2022-02-11T21:48:42.80047Z","shell.execute_reply":"2022-02-11T21:48:53.916833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get install -y python3-rosbag","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:48:53.920363Z","iopub.execute_input":"2022-02-11T21:48:53.920656Z","iopub.status.idle":"2022-02-11T21:49:19.405881Z","shell.execute_reply.started":"2022-02-11T21:48:53.920617Z","shell.execute_reply":"2022-02-11T21:49:19.405106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install bagpy","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:49:19.408735Z","iopub.execute_input":"2022-02-11T21:49:19.409059Z","iopub.status.idle":"2022-02-11T21:49:44.783774Z","shell.execute_reply.started":"2022-02-11T21:49:19.40902Z","shell.execute_reply":"2022-02-11T21:49:44.782915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bagpy","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:49:44.786489Z","iopub.execute_input":"2022-02-11T21:49:44.787002Z","iopub.status.idle":"2022-02-11T21:49:53.447966Z","shell.execute_reply.started":"2022-02-11T21:49:44.786959Z","shell.execute_reply":"2022-02-11T21:49:53.447154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install pyquaternion","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:49:53.449804Z","iopub.execute_input":"2022-02-11T21:49:53.450093Z","iopub.status.idle":"2022-02-11T21:50:01.726758Z","shell.execute_reply.started":"2022-02-11T21:49:53.450053Z","shell.execute_reply":"2022-02-11T21:50:01.72588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import rosbag\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport pyquaternion as Q","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:50:01.728747Z","iopub.execute_input":"2022-02-11T21:50:01.72902Z","iopub.status.idle":"2022-02-11T21:50:03.29826Z","shell.execute_reply.started":"2022-02-11T21:50:01.728983Z","shell.execute_reply":"2022-02-11T21:50:03.297506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ROSbagIMUGT(Dataset):\n\n    def __init__(self, bagname, imu_topic, gt_topic, transform=None, imu_seq_length=10, imu_seq_overlap=0.0):\n        # store variables\n        self.imu_topic = None\n        self.gt_topic = None\n        self.cam0_bag = None\n        self.cam0_topic = None\n\n        self.bagname = bagname\n        self.imu_topic = imu_topic\n        self.gt_topic = gt_topic\n\n        self.transform = transform\n        self.imu_seq_length = imu_seq_length\n        self.imu_seq_overlap = imu_seq_overlap\n        self.imu_increment = self.imu_seq_length - round(self.imu_seq_length * self.imu_seq_overlap)\n\n        # this will hold the scaling factors for min-max scaling.\n        self.normalizer = None\n\n        if bagname is not None:\n            self.load_bags()\n            self.precompute_items()\n\n    def __len__(self):\n        # assuming that length corresponds to actual length and not to max index\n        lov = round(self.imu_seq_length * self.imu_seq_overlap)\n        return int((self.imudata.shape[0] - lov) / self.imu_increment)\n\n    def __getitem__(self, index):\n        # only need to index into the precomputed arrays now\n        d_gt_dist_angle = self.d_gt_dist_angle_items[index, :]\n        imudata = self.imudata_items[index, :]\n        delta_yaw = self.delta_yaw_items[index]\n        data = {'imudata': imudata, 'd_gt_dist_angle': d_gt_dist_angle, 'delta_yaw': delta_yaw}\n        # apply transformation if parameters have been set\n        if self.normalizer != None:\n            data = {'imudata': imudata, 'd_gt_dist_angle': d_gt_dist_angle, 'delta_yaw': delta_yaw}\n            data = self.normalizer.transform(data)\n        else:\n            # multiply distances by 1000 so that they are in mm\n            data['d_gt_dist_angle'][0] *= 1000.\n\n        return data['d_gt_dist_angle'], data['imudata'], data['delta_yaw']\n\n    def precompute_items(self):\n        # precompute the arrays with items\n        # return gt difference of xyz positions and attitude as one vector\n        self.imudata_items = np.stack([self.imudata[index*self.imu_increment:(index*self.imu_increment+self.imu_seq_length), :] for index in range(self.__len__())])\n\n        # compute relative distance traveled in x-y plane (need to start from last end position!)\n        d_Txyz = np.stack([np.array(self.gt_translation_xyz[index*self.imu_increment+self.imu_seq_length-1]) -\n                           np.array(self.gt_translation_xyz[max(0, index*self.imu_increment-1)]) for index in range(self.__len__())])\n        distance_items = np.sqrt(np.sum(np.square(d_Txyz[:, 0:2]), axis=1))\n\n        # compute change in attitude\n        q2 = [Q.Quaternion(self.gt_rotation_wxyz[index * self.imu_increment + self.imu_seq_length - 1]) for index in range(self.__len__())]\n        q1 = [Q.Quaternion(self.gt_rotation_wxyz[max(0, index * self.imu_increment - 1)]) for index in range(self.__len__())]\n\n        # change in heading in degrees (i.e. only rotation around world z axis = yaw)\n        for ii in range(self.__len__()):\n            q2[ii][1] = 0\n            q2[ii][2] = 0\n            q1[ii][1] = 0\n            q1[ii][2] = 0\n\n        self.delta_yaw_items = np.stack([(this_q2 * this_q1.conjugate).radians for this_q2, this_q1 in zip(q2, q1)])\n\n        # compute polar angle associated with displacement\n        angle_items = np.arctan2(d_Txyz[:, 1], d_Txyz[:, 0])\n\n        self.d_gt_dist_angle_items = torch.tensor(np.stack((distance_items, angle_items), axis=1), dtype=torch.float64)\n\n    def load_bags(self):\n        # always need IMU and GT\n        bag = rosbag.Bag(self.bagname)\n\n        imus = bag.read_messages(topics=self.imu_topic)\n        gts = bag.read_messages(topics=self.gt_topic)\n\n        # extract all gt poses and timestamps\n        translation_xyz = []\n        rotation_wxyz = []\n        gt_times = []\n\n        for gt in gts:\n            translation_xyz.append([gt.message.pose[2].position.x, gt.message.pose[2].position.y,\n                                    gt.message.pose[2].position.z])\n            rotation_wxyz.append([gt.message.pose[2].orientation.w, gt.message.pose[2].orientation.x,\n                                  gt.message.pose[2].orientation.y, gt.message.pose[2].orientation.z])\n\n        # extract all IMU readings and timestamps\n        accel_xyz = []\n        angvel_xyz = []\n        imu_times = []\n\n        for i, imu in enumerate(imus):\n            imu_timestamp = imu.message.header.stamp.secs + imu.message.header.stamp.nsecs / 1.e9\n            if i == 0:\n                starttime = imu_timestamp\n\n            # keep only readings within specified second limits\n            accel_xyz.append([imu.message.linear_acceleration.x, imu.message.linear_acceleration.y, \\\n                              imu.message.linear_acceleration.z])\n            angvel_xyz.append([imu.message.angular_velocity.x, imu.message.angular_velocity.y,\n                               imu.message.angular_velocity.z])\n            imu_times.append(imu_timestamp)\n\n        self.gt_translation_xyz = np.array(translation_xyz)\n        self.gt_rotation_wxyz = np.array(rotation_wxyz)\n        self.imudata = np.concatenate([angvel_xyz, accel_xyz], axis=1)\n        self.imu_times = np.array(imu_times)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-11T21:50:03.299662Z","iopub.execute_input":"2022-02-11T21:50:03.299951Z","iopub.status.idle":"2022-02-11T21:50:03.326806Z","shell.execute_reply.started":"2022-02-11T21:50:03.2999Z","shell.execute_reply":"2022-02-11T21:50:03.326143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.set_printoptions(precision=20)\ntorch.set_default_dtype(torch.float64)\nfrom torch import nn\nfrom os import path\nfrom torch.utils.data import ConcatDataset\nimport time\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:50:03.328868Z","iopub.execute_input":"2022-02-11T21:50:03.329336Z","iopub.status.idle":"2022-02-11T21:50:03.344004Z","shell.execute_reply.started":"2022-02-11T21:50:03.329294Z","shell.execute_reply":"2022-02-11T21:50:03.343271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_DIR = './'\nTRAINING_BAGS_DIR = '../input/rosbags/'\nTESTING_BAGS_DIR = '../input/rosbags/'\nNUM_EPOCHS = 100\nMINIBATCH_SIZE = 32\nIMU_SEQUENCE_LENGTH = 10\nHIDDEN_SIZE = 128\nNORMALIZE_DATA = True\n\nclass MinMaxNormalizer:\n    def __init__(self, feature_range=(-1, 1)):\n        self.transform_dict = None\n        self.feature_range = feature_range\n\n    def fit_transform(self, datasets):\n        # loop through datasets and determine global min and max value per feature\n        rawdata = np.concatenate([dset.imudata for dset in datasets.datasets])\n        imu_dmax = rawdata.max(axis=0)\n        imu_dmin = rawdata.min(axis=0)\n\n        rawdata = np.concatenate([dset.d_gt_dist_angle_items for dset in datasets.datasets])\n        distangle_dmax = rawdata.max(axis=0)\n        distangle_dmin = rawdata.min(axis=0)\n\n        rawdata = np.concatenate([dset.delta_yaw_items for dset in datasets.datasets])\n        deltayaw_dmax = rawdata.max()\n        deltayaw_dmin = rawdata.min()\n\n        self.transform_dict = {'imudata': [imu_dmin, imu_dmax],\n                               'dist_angle': [distangle_dmin, distangle_dmax],\n                               'delta_yaw': [deltayaw_dmin, deltayaw_dmax]}\n\n    def transform(self, data):\n        data['imudata'] = self.__apply_transform__(data['imudata'], self.transform_dict['imudata'])\n        data['d_gt_dist_angle'] = self.__apply_transform__(data['d_gt_dist_angle'], self.transform_dict['dist_angle'])\n        data['delta_yaw'] = self.__apply_transform__(data['delta_yaw'], self.transform_dict['delta_yaw'])\n\n        return data\n\n    # undoes transformation (assumes torch tensor input)\n    def inverse_transform(self, datascaled, dminmax):\n        data = torch.tensor((dminmax[1] - dminmax[0]), device=datascaled.device) * (datascaled - self.feature_range[0]) / (self.feature_range[1] - self.feature_range[0]) \\\n               + torch.tensor(dminmax[0], device=datascaled.device)\n        return data\n\n    def __apply_transform__(self, data, dminmax):\n        datastd = (data - dminmax[0]) / (dminmax[1] - dminmax[0])\n        datascaled = datastd * (self.feature_range[1] - self.feature_range[0]) + self.feature_range[0]\n\n        return datascaled\n\n\ndef load_normalizer(dataset, normalizer):\n    try:\n        for dset in dataset.datasets:\n            dset.normalizer = normalizer\n    except:\n        dataset.normalizer = normalizer\n\n    return dataset\n\n\nclass IPPU(nn.Module):\n    def __init__(self):\n        super(IPPU, self).__init__()\n\n        self.LSTM = nn.LSTM(input_size=6, hidden_size=HIDDEN_SIZE, num_layers=2, batch_first=True,\n                            bidirectional=True, dropout=0.50)\n        # TODO: infer input channel size from input data\n        self.fc1 = nn.Linear(HIDDEN_SIZE, 2)\n\n    def forward(self, imudata):\n        _, (h_out, _) = self.LSTM(imudata)\n        x = self.fc1(h_out[-1, :])\n        return x\n\n\ndef compute_loss(outputs, gt_data, criterion):\n    # compute loss according to criterion\n    loss = criterion(outputs, gt_data)\n\n    return loss\n\n\ndef train_one_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n\n    print('Training IMU')\n\n    running_loss = 0.0\n\n    for i, data in enumerate(dataloader, 0):\n        imu_data = data[1]\n        imu_data = imu_data.to(device, torch.float64)\n        gt_delta = data[0]\n        gt_delta = gt_delta.to(device)\n\n        outputs = model(imu_data)\n\n        loss = compute_loss(outputs, gt_delta, criterion)\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # if (i % 100 == 0) & (i > 0):\n        #     print('-----------------------------------------')\n        #     print(\"IPPU: Loss at iteration %d: %.5f\" % (i, loss))\n\n    print('Epoch loss average: {:.4f}'.format(running_loss/(i+1)))\n    print('-' * 10)\n\n    return running_loss/(i+1)\n\n\ndef test_model(model, dataloader, criterion, device):\n    model.eval()\n\n    print('Testing model')\n\n    running_loss = 0.0\n\n    for i, data in enumerate(dataloader, 0):\n        imu_data = data[1]\n        imu_data = imu_data.to(device, torch.float64)\n        gt_delta = data[0]\n        gt_delta = gt_delta.to(device)\n\n        outputs = model(imu_data)\n\n        loss = compute_loss(outputs, gt_delta, criterion)\n\n        running_loss += loss.item()\n\n        # if (i % 100 == 0) & (i > 0):\n        #     print('-----------------------------------------')\n        #     print(\"IPPU: Loss at iteration %d: %.5f\" % (i, loss))\n\n    print('Test loss average: {:.4f}'.format(running_loss/(i+1)))\n    print('-' * 10)\n\n    return running_loss/(i+1)\n\n\nif __name__ == '__main__':\n    # check if we're running on GPU or CPU\n    device = torch.device(\"cuda\")\n    #device = torch.device(\"cpu\")\n\n    # load global variables to device\n    model = IPPU()\n    model.to(device)\n\n    # define train dataset, data_transforms, dataset and dataloader\n    trainsets = [TRAINING_BAGS_DIR + 'ROS_BAG_1.bag',\n                 TRAINING_BAGS_DIR + 'ROS_BAG_2.bag',\n                 TRAINING_BAGS_DIR + 'ROS_BAG_3.bag',\n                 TRAINING_BAGS_DIR + 'ROS_BAG_4.bag',\n                 TRAINING_BAGS_DIR + 'ROS_BAG_5.bag',\n                 TRAINING_BAGS_DIR + 'ROS_BAG_6.bag',\n                 TRAINING_BAGS_DIR + 'ROS_BAG_7.bag',\n                 TRAINING_BAGS_DIR + 'ROS_BAG_8.bag',\n                 TRAINING_BAGS_DIR + 'ROS_BAG_9.bag',\n                 TRAINING_BAGS_DIR + 'ROS_BAG_10.bag']\n\n    trainset = ConcatDataset([ROSbagIMUGT(dataset, \"/imu\", \"/gazebo/model_states\", imu_seq_length=IMU_SEQUENCE_LENGTH)\n                              for dataset in trainsets])\n\n    # trainset = ROSbagIMUGT(TRAINING_BAGS_DIR + 'ROS_BAG_1.bag', \"/imu\", \"/gazebo/model_states\",\n    #                       imu_seq_length=IMU_SEQUENCE_LENGTH)\n    # define test dataset\n    testset = ROSbagIMUGT(TESTING_BAGS_DIR + 'ROS_BAG_11.bag', \"/imu\", \"/gazebo/model_states\",\n                          imu_seq_length=IMU_SEQUENCE_LENGTH)\n\n    # normalize training and testing datasets if desired\n    if NORMALIZE_DATA:\n        normalizer = MinMaxNormalizer()\n        normalizer.fit_transform(trainset)\n        trainset = load_normalizer(trainset, normalizer)\n        testset = load_normalizer(testset, normalizer)\n        file = open(path.join(OUTPUT_DIR, 'MinMaxNormalizer_' + time.strftime('%d%b%Y_%H%M%S') + '.pkl'), 'wb')\n        pickle.dump(normalizer, file)\n        file.close()\n\n    # create the dataloaders for training and testing data\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=MINIBATCH_SIZE,\n                                              shuffle=True, drop_last=True, num_workers=0)\n\n    testloader = torch.utils.data.DataLoader(testset, batch_size=MINIBATCH_SIZE,\n                                             shuffle=True, drop_last=True, num_workers=0)\n\n    for param in model.parameters():\n        param.requires_grad = True\n    params = [p for p in model.parameters() if p.requires_grad]\n\n    criterion = nn.MSELoss(reduction='mean')\n    optimizer = torch.optim.Adam(params, lr=0.001)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.1)\n\n    train_losses = []\n    test_losses = []\n    for ii in range(NUM_EPOCHS):\n        print('Epoch {}/{}'.format(ii, NUM_EPOCHS - 1))\n        print('-' * 20)\n        start = time.time()\n        train_loss = train_one_epoch(model, trainloader, criterion, optimizer, device)\n        train_losses.append(train_loss)\n        test_loss = test_model(model, testloader, criterion, device)\n        test_losses.append(test_loss)\n        lr_scheduler.step()\n        stop = time.time()\n        print('Time per epoch: %.5f' % (stop - start))\n\n    # save model\n    model_name = path.join(OUTPUT_DIR, 'IPPU_' + time.strftime('%d%b%Y_%H%M%S') + '.ptm')\n    torch.save(model.state_dict(), model_name)\n    print('Model saved as: ' + model_name)\n\n    # plot train - test losses\n    plt.figure()\n    plt.plot(train_losses, label='Train')\n    plt.plot(test_losses, label='Test')\n    plt.legend()\n    plt.xlabel('Epochs')\n    plt.title('Average losses')\n    plt.savefig(model_name.split('.ptm')[0]+'_losses.png')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:50:03.345595Z","iopub.execute_input":"2022-02-11T21:50:03.345901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.__version__)\nprint(torch.cuda.is_available())","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}